---
typora-copy-images-to: E:\deep L\img
---

# 中期文档

## 一、背景摘要

  在图像处理和模式识别中，用高维特征描述图片时内容往往有特征冗余和存在噪声干扰等问题.


### 1.1 特征选择基本概念

  一个典型的机器学习任务，是通过样本的特征来预测样本所对应的值。
在实现机器学习任务中，获得数据之后通常首先进行特征选择。即“数据预处理”。
特征选择可以看作一个搜索寻优问题，即选择最优的特征子集。

  - **冗余控制** -- 特征选择的一个重要要求



## 二、研究路线

### 2.1 冗余度分析（redundancy control）

#### 2.1.1 标准化互信息（NMI）


  一种非线性方法测量相关性

- **定义**：

  $I(X;Y) =  $

  ![互信息](E:\deep L\img\互信息.png)

- **代码如下**：
```python
from sklearn import metrics

if __name__ == '_main_':
    A = [1, 1, 1, 2]
    B = [2, 2, 2, 1]
    result_NMI = metrics.normalized_mutual_info_score(A, B)
    print('result_NMU', result_NMI)
```

- **内部函数实现**：

```python
def normalized_multual_info_score(
					labels_true, labels_pred,
					average_method='warn' )：
	
```



> 参数：
 >> label_true {ndarray}  测试数据
 >> label_pred {ndarray}  预测数据

---

###### ***计算结果***

   .xlsx

结果分析：
1. 
2. 


### 2.2 基于对称不确定性的冗余度测度

####  2.2.1对称不确定性（SU）

参考论文：
> *《基于对称不确定性和SVM递归特征消除的信息基因选择方法》*
>
> > *叶明全，高凌云，伍长荣，万春园*


- **定义**：

给定2个随机变量 X 和 Y，X和Y的对称不确定性表示为：
![SU%E5%85%AC%E5%BC%8F.png](attachment:SU%E5%85%AC%E5%BC%8F.png)

> H(x) -- 信息熵
> H(X|Y) -- 条件熵
> IG（X|Y）= H(X) - H(X|Y) -- 信息增益
>
> > 一定程度上反应X和Y的相关性，但考虑到变量单位和取值范围等因素影响，需要对其进行同质化，结果即SU（X,Y）

- **冗余判断标准**

用C表示分类标签，若要判断一对特征 Fi, Fj是否存在冗余，则考虑
SU(Fi, C), SU(Fj, C), SU(Fi, Fj)的大小关系，当
![SU%E5%88%A4%E6%96%AD.png](attachment:SU%E5%88%A4%E6%96%AD.png)
即Fi比Fj与标签有更强相关性且Fi与Fj(相对)高度相关时，Fj被认为冗余


- **计算**


(1)相对功率和绝对功率各特征对于分类标签的对称不确定性

(2)相对功率各特征和绝对功率各特征的对称不确定性
再依据相应的标准判断其冗余程度


----
##### - 将连续数据直接看作离散数据
分子MI  -- 直接代入
分母H（X） -- 频率当概率直接代入

###### ***计算结果***

（没找到> <,  全看记忆 P31）
1. 没有对数据进行任何处理
2. IG（X|Y）在数值上等于互信息（？）
3. 信息熵使用离散方式代入数值进行计算



- ***问题所在***

由于把数值当成离散值，用频率估计概率P(x)的方法欠妥，考虑用其他方法估计P(x)和P(x|y)

#####  - 离散化处理  MDLP

- 定义：
IG（信息增益）:在数值上等于互信息，采用离散化互信息代替

H(X)和H(Y) （信息熵）：离散化计算



- 过程：

![mdlp%E8%BF%87%E7%A8%8B.png](attachment:mdlp%E8%BF%87%E7%A8%8B.png)

###### ***计算结果***

1.程序运行结果上存在以下情况：

1）由于存在人工决定的阈值，数值离散化程度实际上有部分先验因素影响，从目前选定的阈值来看，离散化程度并不高，但应该如何调整仍待讨论

2）由于算法递归层数较深，在部分特征值上的离散化运行报错


2.关于离散化方法的讨论：

1）由于信息熵和互信息的性质，其数值大小对离散化程度敏感，离散化的方法对离散熵数值的影响是显著的

2）基于1）中提到的性质，想要用离散熵验证kde熵的想法无法通过直接比较熵的数值大小来实现

3）现阶段的想法是直接比较离散化计算的SU与kde方法计算的SU，但是由于之前提到的代码运行问题，可能要考虑简单的等宽离散等方法

...待补充


#### 2.2.2核密度、估计（KDE）


- **定义**：
是用于估计概率密度函数的非参数方法


思路：通过核函数将每个数据点的数据+带宽当作核函数的参数，得到N个核函数，再线性叠加就形成了核密度的估计函数，归一化后就是概率密度函数了


- **公式**：
![kde%E5%AE%9A%E4%B9%89.png](attachment:kde%E5%AE%9A%E4%B9%89.png)

>f为概率密度函数
>K（.）为核函数 


- **具体代码**：

```python
  from sklearn.neighbors import kde
  
  def h1decision(b):
      """
      计算带宽
      
      参数：
          b {ndarray}  一维数组
      
      返回值：
          res {int}   这组数据带宽
      """
      # 这里使用样本标准差
      a1 = np.std(b,ddof=1)
      l = len(b)
      tmp = math.pow(l,-0.2)
      res = 1.05*a1*tmp
      return res
  
  
  nyse2 = pd.read_excel('ab.xlsx', usecols=[1])
  b = use.dTl(nyse2)  # df-->ndarray
  b1 = nyse2.values  # 二维数组
  h3 = use.h1decision(b)  # 带宽
  
  logpy = kde.KernelDensity(kernel='gaussian', bandwidth=h3).fit(b1).score_samples(b1)
```


参数解释
>a.score_samples(X)
>
>>返回的是点x对应概率的log值(计算模型下的总log概率)，要使用exp求指数还原


>带宽选择 h = c * N^(-1/5)
>
>> 对正态分布 c = 1.05 * 标准差，作为参考值

##### - 连续数据处理
- 处理方法：
 分母H(X) 使用kde算法估计密度概率


- - **归一化处理**
目的：kde函数是N个核函数线性叠加的结果，为了保证得出P(X)的取值在(0~1)内。

代码如下

    def minmax(X):
        """
        最大最小值标准化【0，1】
        :param X: 需要标准化的二维数组ndarray
        :return: res :标准化后的一维数组ndarray
        """
        x = X.reshape(len(X), 1)
        min_max_scale = preprocessing.MinMaxScaler(feature_range=(0.00000000001, 1))
        tmp = min_max_scale.fit_transform(x)
        res = tmp.flatten()
        return res


​        
​        
​        
​        

----


###### ***计算结果***

1. 分子依旧采用互信息（保留小数点后4位）方式计算
2. 分母熵使用kde进行计算后，最后标准化熵值


![%E8%BF%9B%E5%BA%A61.png](attachment:%E8%BF%9B%E5%BA%A61.png)

*问题*

1. 分子分母不一致处理，合理性待讨论。
2. 实际所用kde估计熵为 将kde所求的概率密度函数直接当作概率计算熵
3. 将熵进行了归一化



##### - 均KDE计算

参考论文：

> *《混合数据的核密度估计熵与快速的贪心特征选择算法》*
>
> > *张靖红*

- KDE熵:根据连续随机变量的实例概率$\int_{x\in X}\hat{p}(x)dx=\frac{1}{n}$，推导如下公式

- 公式：
![kde%E7%86%B5.png](attachment:kde%E7%86%B5.png)


- KDE条件熵

- 公式：
![kde%E6%9D%A1%E4%BB%B6%E7%86%B5.png](attachment:kde%E6%9D%A1%E4%BB%B6%E7%86%B5.png)

- 分析：

1）论文给出的D为标签变量，即只给出了特征对于标签的信息增益计算方法，而特征之间的信息增益计算方法未涉及,此外，条件概率的kde计算方法同样是针对特征和标签间的。（区别：标签是严格的离散变量，而我们计算的特征是连续变量）

2）关于1）会导致的问题：如果直接将条件概率公式运用到特征间条件熵计算：$$p(c|x)=\frac{\sum_{i\in I_{c}}\phi(x-x_i,h)}{\sum_{k=1}^{N}\sum_{i\in I_{k}}\phi(x-x_i,h)}$$ 由于其计算方法本身的特点,我们需要对特征x按特征c的取值进行划分，分别对每一个划分段进行核密度估计，这样的划分对于取值数达到十万级别的连续型数据是不能接受的，同组内过小的数据量也会让kde失去意义

3）对于1）中的问题，我们尝试了两方面的解决方法：a.推导适用于连续特征间条件熵计算的公式 b.计算时，将对应标签特征的特征值离散化处理，这些处理方法又导致了新的问题，我们会对这两个方法做后续讨论：

-  疑问：

1）连续随机变量的实例概率$\int_{x\in X}\hat{p}(x)dx=\frac{1}{n}$，这个公式是张[]的论文里根据Nojun Kwak[]里关于kde条件概率公式推导出的，并没有经过严格的证明，合理性尚且存疑，且后续的一些实验结果表明这个推导（或者近似计算）似乎并不正确

2）如果上述kde熵计算公式被确认不可靠，那么我们关于信息熵的计算方法似乎只剩下离散化一条路可选，这一点需要格外注意

----

###### 尝试方法1.KDE条件熵H(X|Y)（两特征间）的尝试推导

两个推导：

- 1:

![IG%E6%8E%A8%E5%AF%BC1.png](attachment:IG%E6%8E%A8%E5%AF%BC1.png)

- 2:
![IG%E6%8E%A8%E5%AF%BC2.png](attachment:IG%E6%8E%A8%E5%AF%BC2.png)

- 3:
**尝试的条件熵推导结果：**
![%E6%9D%A1%E4%BB%B6%E7%86%B5%E6%8E%A8%E5%AF%BC.png](attachment:%E6%9D%A1%E4%BB%B6%E7%86%B5%E6%8E%A8%E5%AF%BC.png)

实验结果：得出的条件熵并不符合期望（应该是一个比单变量信息熵要小的数值），问题可能出在信息熵，或条件熵上

----

###### 尝试方法2.部分离散化

顾名思义，在计算条件熵p(y|x)时，当作计算p(c|x)处理，即把特征y离散化作为标签处理：

实验结果：目前与熵值的大小关系符合定义，能够得出合法的SU数值，但正确性仍待考量

分析：

1）依照尽量减少信息损失的原则，这样的混用方法似乎可以理解，但是从理论上证明其合理性相当困难

2）一旦用到离散化，则需要考虑离散化方法问题，理由仍然是熵对离散化程度的敏感性

3）关于离散化程度的考量：最优离散化方法其实是一个np难问题，故去寻找一个最佳答案似乎并不明智，目前的想法是，将条件概率的数值控制在其有意义的区间内，即可以接受。理由是我们最后并不会直接把计算出的SU数值用作特征选择，而是用到其大小关系，这一点其实基本不会收到离散化程度的影响。

